min(D3$PhysProx)
mean(D3$PhysProx)
data <- merge(x=D3, y=D2, by.x="SOC", by.y="OCC_CODE")
head(data, n=15)
tail(data)
dim(data)
sum(data$TOT_EMP)
min(data$TOT_EMP)
data$EMP_PERC <- data$TOT_EMP/sum(data$TOT_EMP)
sum(data$EMP_PERC)
data$EMPsynthetic <- round(data$EMP_PERC * 200000)
sum(data$EMPsynthetic)
#data$EMPsynthetic[s] <- data$EMPsynthetic[s] + 1
sum(data$EMPsynthetic)
min(data$EMPsynthetic)
max(data$EMPsynthetic)
data$OCC_TITLE[which(data$EMPsynthetic==0)]
occupation <- list()
degree <- list()
for(i in 1:nrow(data)){
occupation[[i]] <- rep(x=data$SOC[i], times=data$EMPsynthetic[i])
degree[[i]] <- rep(x=data$PhysProx[i], times=data$EMPsynthetic[i])
}
occupation <- unlist(occupation)
length(occupation)
degree <- unlist(degree)
length(degree)
mean(degree)
Proximity <- read.csv("/Users/demetrisavraam/Downloads/PCA (3).csv")    # run on laptop
#Proximity <- read.csv("/home/demetris/WFH/PCA (3).csv")                # run on cluster
head(Proximity)
dim(Proximity)
hist(Proximity$nContact)
mean(Proximity$nContact)
max(Proximity$nContact)
min(Proximity$nContact)
# load the required packages
library(dplyr)
library(igraph)
NYworkforce <- read.csv("/Users/demetrisavraam/Downloads/msadf.csv")   # run on laptop
#NYworkforce <- read.csv("/home/demetris/WFH/msadf.csv")               # run on cluster
head(NYworkforce)
dim(NYworkforce)
D2 <- NYworkforce[which(NYworkforce$OCC_GROUP=='detailed'), c('OCC_CODE','OCC_TITLE','TOT_EMP')]
rownames(D2) <- seq(1:nrow(D2))
dim(D2)
head(D2, n=10)
D2$TOT_EMP <- as.numeric(as.character(D2$TOT_EMP))
#OUT3 <- D2[c(which(is.na(D2$TOT_EMP)==TRUE)),1:2]
#write.csv(OUT3, "/Users/demetrisavraam/Desktop/out3.csv")
D2 <- D2[-c(which(is.na(D2$TOT_EMP)==TRUE)),]
dim(D2)
min(D2$TOT_EMP)
sum(D2$TOT_EMP)
Proximity <- read.csv("/Users/demetrisavraam/Downloads/PCA (3).csv")    # run on laptop
#Proximity <- read.csv("/home/demetris/WFH/PCA (3).csv")                # run on cluster
head(Proximity)
dim(Proximity)
hist(Proximity$nContact)
mean(Proximity$nContact)
Proximity2 <- Proximity
Proximity2$SOC <- substr(as.character(Proximity2$Code), 1, nchar(as.character(Proximity2$Code))-3)
D3 <- Proximity2 %>% group_by(SOC) %>% summarise(PhysProx=mean(nContact))
D3 <- as.data.frame(D3)
D3$PhysProx <- round(D3$PhysProx)
dim(D3)
head(D3)
max(D3$PhysProx)
min(D3$PhysProx)
mean(D3$PhysProx)
data <- merge(x=D3, y=D2, by.x="SOC", by.y="OCC_CODE")
head(data, n=15)
tail(data)
dim(data)
#OUT1 <- Proximity2[-which(Proximity2$SOC %in% D2$OCC_CODE),3:4]
#OUT2 <- D2[-which(D2$OCC_CODE %in% Proximity2$SOC),1:2]
#write.csv(OUT1, "/Users/demetrisavraam/Desktop/out1.csv")
#write.csv(OUT2, "/Users/demetrisavraam/Desktop/out2.csv")
sum(data$TOT_EMP)
min(data$TOT_EMP)
data$EMP_PERC <- data$TOT_EMP/sum(data$TOT_EMP)
sum(data$EMP_PERC)
data$EMPsynthetic <- round(data$EMP_PERC * 200000)
sum(data$EMPsynthetic)
#s <- sample(1:nrow(data), size=3)
#s <- 3
#data$EMPsynthetic[s] <- data$EMPsynthetic[s] + 1
sum(data$EMPsynthetic)
min(data$EMPsynthetic)
max(data$EMPsynthetic)
data$OCC_TITLE[which(data$EMPsynthetic==0)]
occupation <- list()
degree <- list()
for(i in 1:nrow(data)){
occupation[[i]] <- rep(x=data$SOC[i], times=data$EMPsynthetic[i])
degree[[i]] <- rep(x=data$PhysProx[i], times=data$EMPsynthetic[i])
}
occupation <- unlist(occupation)
length(occupation)
degree <- unlist(degree)
length(degree)
mean(degree)
nodes <- as.data.frame(cbind(occupation, degree))
head(nodes)
nodes$worker_id <- sprintf("w%s",seq(1:nrow(nodes)))
head(nodes)
tail(nodes)
nodes$occupation <- as.character(nodes$occupation)
nodes$degree <- as.numeric(as.character(nodes$degree))
head(nodes)
#write.csv(nodes, "/Users/demetrisavraam/Dropbox/Max Planck Institute/WFH Project/WorkersOccupation&Degree.csv")
median(Proximity$nContact)
Nodes <- read.csv('/Users/demetrisavraam/Dropbox/University of Cyprus/Project Management Networks/Data/CleanedNodes.csv')
Nodes <- Nodes[,c(2,3)]
head(Nodes)
# read the data
Edges <- read.csv('/Users/demetrisavraam/Dropbox/University of Cyprus/Project Management Networks/Data/CleanedEdges.csv')
Edges <- Edges[,c(2,3)]
dim(Edges)
Nodes <- read.csv('/Users/demetrisavraam/Dropbox/University of Cyprus/Project Management Networks/Data/CleanedNodes.csv')
Nodes <- Nodes[,c(2,3)]
links=Edges
nodes=Nodes
# Compute out/in degree
outDegree <- table(links$Source)
inDegree <- table(links$Target)
outDegree
nodesOut <- data.frame(cbind('taskID'=names(outDegree), 'outDegree'=as.numeric(outDegree)))
nodesOut <- merge(x=nodesOut, y=nodes, by.x='taskID', by.y='taskID')
nodesOut
# Compute out/in degree
outDegree <- table(links$Source)
inDegree <- table(links$Target)
# Create new tables which contain the Task ID, the respective degree and the
# start date. Note that any task containing no in (or equally, out) degree
# is subseqenlty removed
nodesOut <- data.frame(cbind('taskID'=names(outDegree), 'outDegree'=as.numeric(outDegree)))
nodesOut <- merge(x=nodesOut, y=nodes, by.x='taskID', by.y='taskID')
colnames(nodesOut) <- c('taskID', 'outDegree', 'startDate')
nodesOut$taskID <- as.character(nodesOut$taskID)
nodesOut$outDegree <- as.numeric(as.character(nodesOut$outDegree))
nodesOut$startDate <- as.numeric(as.Date(as.character(nodesOut$startDate), format="%d/%m/%Y"))
nodesOut <- nodesOut[which(nodesOut$outDegree > 0),] # remove any rows with outDegree equal to zero
rownames(nodesOut) <- seq(1:nrow(nodesOut))
nodesIn <- as.data.frame(cbind('taskID'=names(inDegree), 'inDegree'=as.numeric(inDegree)))
nodesIn <- merge(x=nodesIn, y=nodes, by.x='taskID', by.y='taskID')
colnames(nodesIn) <- c('taskID', 'inDegree', 'startDate')
nodesIn$taskID <- as.character(nodesIn$taskID)
nodesIn$inDegree <- as.numeric(as.character(nodesIn$inDegree))
nodesIn$startDate <- as.numeric(as.Date(as.character(nodesIn$startDate), format="%d/%m/%Y"))
nodesIn <- nodesIn[which(nodesIn$inDegree > 0),] # remove any rows with inDegree equal to zero
rownames(nodesIn) <- seq(1:nrow(nodesIn))
linksNull <- data.frame(Source=character(), Target=character())
linksNull
nodesIn
#
# PS - note that under this scheme, the heterogeneity of the node degree is preserved
#
reshuffNodes <- function(links, nodes){
# Compute out/in degree
outDegree <- table(links$Source)
inDegree <- table(links$Target)
# Create new tables which contain the Task ID, the respective degree and the
# start date. Note that any task containing no in (or equally, out) degree
# is subseqenlty removed
nodesOut <- data.frame(cbind('taskID'=names(outDegree), 'outDegree'=as.numeric(outDegree)))
nodesOut <- merge(x=nodesOut, y=nodes, by.x='taskID', by.y='taskID')
colnames(nodesOut) <- c('taskID', 'outDegree', 'startDate')
nodesOut$taskID <- as.character(nodesOut$taskID)
nodesOut$outDegree <- as.numeric(as.character(nodesOut$outDegree))
nodesOut$startDate <- as.numeric(as.Date(as.character(nodesOut$startDate), format="%d/%m/%Y"))
nodesOut <- nodesOut[which(nodesOut$outDegree > 0),] # remove any rows with outDegree equal to zero
rownames(nodesOut) <- seq(1:nrow(nodesOut))
nodesIn <- as.data.frame(cbind('taskID'=names(inDegree), 'inDegree'=as.numeric(inDegree)))
nodesIn <- merge(x=nodesIn, y=nodes, by.x='taskID', by.y='taskID')
colnames(nodesIn) <- c('taskID', 'inDegree', 'startDate')
nodesIn$taskID <- as.character(nodesIn$taskID)
nodesIn$inDegree <- as.numeric(as.character(nodesIn$inDegree))
nodesIn$startDate <- as.numeric(as.Date(as.character(nodesIn$startDate), format="%d/%m/%Y"))
nodesIn <- nodesIn[which(nodesIn$inDegree > 0),] # remove any rows with inDegree equal to zero
rownames(nodesIn) <- seq(1:nrow(nodesIn))
linksNull <- data.frame(Source=character(), Target=character())
for(i in 1:(nrow(links)+150)){
if(nrow(nodesOut) >=1 & nrow(nodesIn) >=1){
# Start from the top
idxOut <- nodesOut$taskID[1]
lidxIn <- nodesIn$taskID
TaskID_In <- lidxIn[lidxIn != idxOut] # remove the idxOut node from the vector of idxIn nodes if it exist
idxIn <- sample(TaskID_In, 1) # select one node from lidxIn randomly
if (length(which(linksNull$Source==idxOut & linksNull$Target==idxIn))==0){
linksNull <- rbind(linksNull, c(idxOut, idxIn))
colnames(linksNull) <- c("Source", "Target")
linksNull$Source <- as.character(linksNull$Source)
linksNull$Target <- as.character(linksNull$Target)
nodesOut$outDegree[which(nodesOut$taskID == idxOut)] <- nodesOut$outDegree[which(nodesOut$taskID == idxOut)] - 1
nodesIn$inDegree[which(nodesIn$taskID == idxIn)] <- nodesIn$inDegree[which(nodesIn$taskID == idxIn)] - 1
}else{
linksNull <- linksNull
}
nodesIn <- nodesIn[which(nodesIn$inDegree > 0),] # update nodesIn by removing any rows with inDegree equal to zero
rownames(nodesIn) <- seq(1:nrow(nodesIn))
nodesOut <- nodesOut[which(nodesOut$outDegree > 0),] # update nodesOut by removing any rows with outDegree equal to zero
rownames(nodesOut) <- seq(1:nrow(nodesOut))
print(nrow(nodesIn))
}
}
return(linksNull)
}
# read the data
Edges <- read.csv('/Users/demetrisavraam/Dropbox/University of Cyprus/Project Management Networks/Data/CleanedEdges.csv')
Edges <- Edges[,c(2,3)]
dim(Edges)
Nodes <- read.csv('/Users/demetrisavraam/Dropbox/University of Cyprus/Project Management Networks/Data/CleanedNodes.csv')
Nodes <- Nodes[,c(2,3)]
out <- reshuffNodes(links=Edges, nodes=Nodes)
# read the data
Edges <- read.csv('/Users/demetrisavraam/Dropbox/University of Cyprus/Project Management Networks/Data/CleanedEdges.csv')
head(Edges)
Nodes <- read.csv('/Users/demetrisavraam/Dropbox/University of Cyprus/Project Management Networks/Data/CleanedNodes.csv')
head(Nodes)
dim(Edges)
# read the data
Edges <- read.csv('/Users/demetrisavraam/Dropbox/University of Cyprus/Project Management Networks/Data/CleanedEdges.csv')
Edges <- Edges[,c(2,3)]
rm(list=ls())
# load libraries
library(dplyr)
library(igraph)
# define the path to read the initial A14 data
setwd('/Users/demetrisavraam/Dropbox/University of Cyprus/Project Management Networks/A14 Data/')
# read the edgeList data
edges <- read.csv(file = 'edgeList.csv',header = TRUE)
head(edges)
tail(edges)
dim(edges) # 1036477 links
# keep only distinct links
edges <- distinct(edges)
rownames(edges) <- seq(1:nrow(edges))
head(edges)
tail(edges)
dim(edges) # 546380 links
length(unique(edges$Source))  # to see how many unique nodes we have as sources (298636 sources)
length(unique(edges$Target))  # to see how many unique nodes we have as targets (306944 targets)
length(unique(c(unique(edges$Source),unique(edges$Target)))) # to see how many unique nodes we have in general (315268 unique tasks in edges)
# read the tsks data
tsks <- read.csv(file='tsks_18_12_2019.csv', header=TRUE, na.strings='')
dim(tsks)
head(tsks)
tail(tsks)
length(unique(tsks$ObjectId))  # (491495 unique tasks)
tsksSlim <- tsks[,c('ObjectId','Baseline1StartDate','Baseline1FinishDate','ActualStartDate','ActualFinishDate')]
tsksSlim <- tsksSlim[complete.cases(tsks),]
rownames(tsksSlim) <- seq(1:nrow(tsksSlim))
head(tsksSlim, n=10)
dim(tsksSlim)
tsksSlim$delay <- as.Date(as.character(tsksSlim$ActualFinishDate), format="%d/%m/%Y") - as.Date(as.character(tsksSlim$Baseline1FinishDate), format="%d/%m/%Y")  # calculate delay measure
# convert delay to binary 1=delay 0=no delay
tsksSlim$delayCat <- rep(NA, length(tsksSlim$delay))
tsksSlim$delayCat[which(tsksSlim$delay > 0)] <- 1
tsksSlim$delayCat[which(tsksSlim$delay <= 0)] <- 0
# remove dublicated rows
TSKS <- distinct(tsksSlim)
head(TSKS)
dim(TSKS)
# create a subset from the dataset "edges" to keep only links that we have info in the dataset "TSKS"
edgesSub1 <- edges[which((edges$Source %in% TSKS$ObjectId) & (edges$Target %in% TSKS$ObjectId)),]
dim(edgesSub1) # 111550 links
rownames(edgesSub1) <- seq(1:nrow(edgesSub1))
head(edgesSub1)
length(unique(c(unique(edgesSub1$Source),unique(edgesSub1$Target)))) # to see how many unique nodes we have in general (74731 unique tasks in edges)
edgesSub3 <- edgesSub1
rm(list=ls())
# load libraries
library(dplyr)
library(igraph)
# define the path to read the initial A14 data
setwd('/Users/demetrisavraam/Dropbox/University of Cyprus/Project Management Networks/A14 Data/')
# read the edgeList data
edges <- read.csv(file = 'edgeList.csv',header = TRUE)
head(edges)
tail(edges)
dim(edges) # 1036477 links
# keep only distinct links
edges <- distinct(edges)
rownames(edges) <- seq(1:nrow(edges))
head(edges)
tail(edges)
dim(edges) # 546380 links
length(unique(edges$Source))  # to see how many unique nodes we have as sources (298636 sources)
length(unique(edges$Target))  # to see how many unique nodes we have as targets (306944 targets)
length(unique(c(unique(edges$Source),unique(edges$Target)))) # to see how many unique nodes we have in general (315268 unique tasks in edges)
# read the tsks data
tsks <- read.csv(file='tsks_18_12_2019.csv', header=TRUE, na.strings='')
dim(tsks)
head(tsks)
tail(tsks)
length(unique(tsks$ObjectId))  # (491495 unique tasks)
tsksSlim <- tsks[,c('ObjectId','Baseline1StartDate','Baseline1FinishDate','ActualStartDate','ActualFinishDate')]
tsksSlim <- tsksSlim[complete.cases(tsks),]
rownames(tsksSlim) <- seq(1:nrow(tsksSlim))
head(tsksSlim, n=10)
dim(tsksSlim)
tsksSlim$delay <- as.Date(as.character(tsksSlim$ActualFinishDate), format="%d/%m/%Y") - as.Date(as.character(tsksSlim$Baseline1FinishDate), format="%d/%m/%Y")  # calculate delay measure
# convert delay to binary 1=delay 0=no delay
tsksSlim$delayCat <- rep(NA, length(tsksSlim$delay))
tsksSlim$delayCat[which(tsksSlim$delay > 0)] <- 1
tsksSlim$delayCat[which(tsksSlim$delay <= 0)] <- 0
# remove dublicated rows
TSKS <- distinct(tsksSlim)
head(TSKS)
dim(TSKS)
# create a subset from the dataset "edges" to keep only links that we have info in the dataset "TSKS"
edgesSub1 <- edges[which((edges$Source %in% TSKS$ObjectId) & (edges$Target %in% TSKS$ObjectId)),]
dim(edgesSub1) # 111550 links
rownames(edgesSub1) <- seq(1:nrow(edgesSub1))
head(edgesSub1)
length(unique(c(unique(edgesSub1$Source),unique(edgesSub1$Target)))) # to see how many unique nodes we have in general (74731 unique tasks in edges)
edgesSub3 <- edgesSub1
# add extra columns in the edgesSub3 to indicate actual delays/no delays of sources and targets
edgesSub3$SourceDelay <- rep(NA, nrow(edgesSub3))
edgesSub3$TargetDelay <- rep(NA, nrow(edgesSub3))
edgesSub3$SourceDelay[which(edgesSub3$Source %in% TSKS$ObjectId[which(TSKS$delayCat==1)])] <- 1
edgesSub3$SourceDelay[which(edgesSub3$Source %in% TSKS$ObjectId[which(TSKS$delayCat==0)])] <- 0
edgesSub3$TargetDelay[which(edgesSub3$Target %in% TSKS$ObjectId[which(TSKS$delayCat==1)])] <- 1
edgesSub3$TargetDelay[which(edgesSub3$Target %in% TSKS$ObjectId[which(TSKS$delayCat==0)])] <- 0
table(edgesSub3$SourceDelay,edgesSub3$TargetDelay)
edgesSub4 <- edgesSub3
# add the columns SourcePlannedEndDate, SourceActualEndDate and TargetPlannedStartDate in edgesSub4
edgesSub4$SourcePlannedEndDate <- c()
edgesSub4$SourceActualEndDate <- c()
edgesSub4$TargetPlannedStartDate <- c()
for (i in 1:nrow(edgesSub4)){
edgesSub4$SourcePlannedEndDate[i] <- as.character(TSKS$Baseline1FinishDate[which(TSKS$ObjectId==edgesSub4$Source[i])])
edgesSub4$SourceActualEndDate[i] <- as.character(TSKS$ActualFinishDate[which(TSKS$ObjectId==edgesSub4$Source[i])])
edgesSub4$TargetPlannedStartDate[i] <- as.character(TSKS$Baseline1StartDate[which(TSKS$ObjectId==edgesSub4$Target[i])])
print(i)
}
head(edgesSub4, n=10)
# convert dates to numbers
edgesSub4$SourcePlannedEndDate.n <- as.Date(as.character(edgesSub4$SourcePlannedEndDate), format="%d/%m/%Y")
edgesSub4$TargetPlannedStartDate.n <- as.Date(as.character(edgesSub4$TargetPlannedStartDate), format="%d/%m/%Y")
head(edgesSub4)
# add the column PlannedTimeDiff in edgesSub4
edgesSub4$PlannedTimeDiff <- edgesSub4$TargetPlannedStartDate.n - edgesSub4$SourcePlannedEndDate.n
head(edgesSub4)
# remove the rows with negative time difference
edgesSub5 <- edgesSub4[which(edgesSub4$PlannedTimeDiff >=0),]
head(edgesSub5)
dim(edgesSub5)
rownames(edgesSub5) <- seq(1:nrow(edgesSub5))
dim(edgesSub4)
# save links and nodes
Edges <- edgesSub4[,c("Source","Target","PlannedTimeDiff")]
dim(Edges)   #
head(Edges)
setwd('/Users/demetrisavraam/Dropbox/University of Cyprus/Project Management Networks/Data/')
write.csv(Edges, file="CleanedEdgesForReshuffleFreeFloats.csv")
rm(list=ls())
#
# This function reshuffles the activity network without considering baseline times
# INPUT: links - n1 x 2 matrix that contains the n1 links
#                (1st column is the 'Source' and 2nd column is the 'Target')
#        nodes - n2 x 2 matrix that contains the n2 unique nodes
#                (1st column is the 'taskID' and 2nd column is the 'baseline.start.date')
#
# OUTPUT: linksNull - contains the new network topology
#
# PS - note that under this scheme, the heterogeneity of the node degree is preserved
#
reshuffFreeFloats <- function(links){
out <- data.frame(Source=links$Source, Target=links$Target, FreeFloat=sample(links$PlannedTimeDiff))
return(out)
}
# read the data
Edges <- read.csv('/Users/demetrisavraam/Dropbox/University of Cyprus/Project Management Networks/Data/CleanedEdgesForReshuffleFreeFloats.csv')
Edges <- Edges[,c(2,4)]
dim(Edges)
for (run in 1:100){
out <- reshuffNodes(links=Edges)
dim(out)
setwd('/Users/demetrisavraam/Dropbox/University of Cyprus/Project Management Networks/Data/NullModelFreeFloat/')
filename <- paste0('EdgesNull', run ,'.csv')
write.csv(out, file=filename)
}
rm(list=ls())
#
# This function reshuffles the activity network without considering baseline times
# INPUT: links - n1 x 2 matrix that contains the n1 links
#                (1st column is the 'Source' and 2nd column is the 'Target')
#        nodes - n2 x 2 matrix that contains the n2 unique nodes
#                (1st column is the 'taskID' and 2nd column is the 'baseline.start.date')
#
# OUTPUT: linksNull - contains the new network topology
#
# PS - note that under this scheme, the heterogeneity of the node degree is preserved
#
reshuffFreeFloats <- function(links){
out <- data.frame(Source=links$Source, Target=links$Target, FreeFloat=sample(links$PlannedTimeDiff))
return(out)
}
# read the data
Edges <- read.csv('/Users/demetrisavraam/Dropbox/University of Cyprus/Project Management Networks/Data/CleanedEdgesForReshuffleFreeFloats.csv')
Edges <- Edges[,c(2,4)]
dim(Edges)
for (run in 1:100){
out <- reshuffFreeFloats(links=Edges)
dim(out)
setwd('/Users/demetrisavraam/Dropbox/University of Cyprus/Project Management Networks/Data/NullModelFreeFloat/')
filename <- paste0('EdgesNull', run ,'.csv')
write.csv(out, file=filename)
}
links=Edges
head(links)
Edges <- Edges[,c(2:4)]
dim(Edges)
# read the data
Edges <- read.csv('/Users/demetrisavraam/Dropbox/University of Cyprus/Project Management Networks/Data/CleanedEdgesForReshuffleFreeFloats.csv')
Edges <- Edges[,c(2:4)]
dim(Edges)
for (run in 1:100){
out <- reshuffFreeFloats(links=Edges)
dim(out)
setwd('/Users/demetrisavraam/Dropbox/University of Cyprus/Project Management Networks/Data/NullModelFreeFloat/')
filename <- paste0('EdgesNull', run ,'.csv')
write.csv(out, file=filename)
}
rm(list=ls())
#
# This function reshuffles the activity network while obeying the constraint of task ordering
# i.e. if task i is connected with task j, then task i starts earlier (or the same day) than task j
#
# INPUT: links - n1 x 2 matrix that contains the n1 links
#                (1st column is the 'Source' and 2nd column is the 'Target')
#        nodes - n2 x 2 matrix that contains the n2 unique nodes
#                (1st column is the 'taskID' and 2nd column is the 'baseline.start.date')
#
# OUTPUT: linksNull - contains the new network topology
#
# PS - note that under this scheme, the heterogeneity of the node degree is preserved
#
reshuffNodes <- function(links, nodes){
# Compute out/in degree
outDegree <- table(links$Source)
inDegree <- table(links$Target)
# Create new tables which contain the Task ID, the respective degree and the
# start date. Note that any task containing no in (or equally, out) degree
# is subseqenlty removed
nodesOut <- data.frame(cbind('taskID'=names(outDegree), 'outDegree'=as.numeric(outDegree)))
nodesOut <- merge(x=nodesOut, y=nodes, by.x='taskID', by.y='taskID')
colnames(nodesOut) <- c('taskID', 'outDegree', 'startDate')
nodesOut$taskID <- as.character(nodesOut$taskID)
nodesOut$outDegree <- as.numeric(as.character(nodesOut$outDegree))
nodesOut$startDate <- as.numeric(as.Date(as.character(nodesOut$startDate), format="%d/%m/%Y"))
nodesOut <- nodesOut[which(nodesOut$outDegree > 0),] # remove any rows with outDegree equal to zero
nodesOut <- nodesOut[(order(nodesOut$startDate, decreasing=TRUE)),]
rownames(nodesOut) <- seq(1:nrow(nodesOut))
nodesIn <- as.data.frame(cbind('taskID'=names(inDegree), 'inDegree'=as.numeric(inDegree)))
nodesIn <- merge(x=nodesIn, y=nodes, by.x='taskID', by.y='taskID')
colnames(nodesIn) <- c('taskID', 'inDegree', 'startDate')
nodesIn$taskID <- as.character(nodesIn$taskID)
nodesIn$inDegree <- as.numeric(as.character(nodesIn$inDegree))
nodesIn$startDate <- as.numeric(as.Date(as.character(nodesIn$startDate), format="%d/%m/%Y"))
nodesIn <- nodesIn[which(nodesIn$inDegree > 0),] # remove any rows with inDegree equal to zero
nodesIn <- nodesIn[(order(nodesIn$startDate, decreasing=TRUE)),]
rownames(nodesIn) <- seq(1:nrow(nodesIn))
# This loop does the following. It choose the task with the latest start date.
# It then identifies all potential tasks that have at least the same start date (and ensures that task i is not in them)
# It then randomly connects to one of those, and the out/in degree in the input matrices is subseqently adjuste to make the new change.
# This process is repeated until no outstanding links are left.
linksNull <- data.frame(Source=character(), Target=character())
for(i in 1:(nrow(links)+150)){
if(nrow(nodesOut) >=1 & nrow(nodesIn) >=1){
# Start from the top
idxOut <- nodesOut$taskID[1]
lidxIn <- nodesIn$taskID[which(nodesIn$startDate > nodesOut$startDate[1])]
#lidxEdg <- as.character(linksNull$Target[which(linksNull$Source=='idxOut')])
TaskID_In <- lidxIn[lidxIn != idxOut] # remove the idxOut node from the vector of idxIn nodes if it exist
#if(length(lidxEdg)>0){
#  TaskID_In <- TaskID_In[TaskID_In != lidxEdg] # remove also any other nodes are already connected with idxOut if they exist
#}
idxIn <- sample(TaskID_In, 1) # select one node from lidxIn randomly
if (length(which(linksNull$Source==idxOut & linksNull$Target==idxIn))==0){
linksNull <- rbind(linksNull, c(idxOut, idxIn))
colnames(linksNull) <- c("Source", "Target")
linksNull$Source <- as.character(linksNull$Source)
linksNull$Target <- as.character(linksNull$Target)
nodesOut$outDegree[which(nodesOut$taskID == idxOut)] <- nodesOut$outDegree[which(nodesOut$taskID == idxOut)] - 1
nodesIn$inDegree[which(nodesIn$taskID == idxIn)] <- nodesIn$inDegree[which(nodesIn$taskID == idxIn)] - 1
}else{
linksNull <- linksNull
}
nodesIn <- nodesIn[which(nodesIn$inDegree > 0),] # update nodesIn by removing any rows with inDegree equal to zero
rownames(nodesIn) <- seq(1:nrow(nodesIn))
nodesOut <- nodesOut[which(nodesOut$outDegree > 0),] # update nodesOut by removing any rows with outDegree equal to zero
rownames(nodesOut) <- seq(1:nrow(nodesOut))
print(nrow(nodesIn))
}
}
return(linksNull)
}
# read the data
Edges <- read.csv('/Users/demetrisavraam/Dropbox/University of Cyprus/Project Management Networks/Data/CleanedEdges.csv')
Edges <- Edges[,c(2,3)]
dim(Edges)
Nodes <- read.csv('/Users/demetrisavraam/Dropbox/University of Cyprus/Project Management Networks/Data/CleanedNodes.csv')
Nodes <- Nodes[,c(2,3)]
for (run in 1:100){
out <- reshuffNodes(links=Edges, nodes=Nodes)
setwd('/Users/demetrisavraam/Dropbox/University of Cyprus/Project Management Networks/Data/NullModelTopology/')
filename <- paste0('EdgesNull', run ,'.csv')
write.csv(out, file=filename)
}
